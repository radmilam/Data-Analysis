Exploratory Graphs
==================

Background - perceptual tasks
-----------------------------
Comparisons between quantitative measurements

Position vs. length

No pie charts - higher error rate compared to bar charts, easier to compare lengths than angles

* Use common scales when possible
* When possible use position comparisons
* Angle comparisons are frequently hard to interpret
* No 3D charts

```{r cache=T}
pData <- read.csv("./data/ss06pid.csv")
```

Boxplots
--------
Quantitative variables - what does distribution look like
* params: col, varwidth, names, horizontal
* line in middle is median
* upper/lower bounds of box are 25th and 75th percentile
* whiskers = 1.5 * 75th percentile, 1.5 * 25th percentile
* if there are values larger or smaller, they'll be represented as dots above or below the whiskers

```{r cache=TRUE}
boxplot(pData$AGEP, col="blue")
```

#### plot informs whether:
* the data has a large number of values spread out or compacted
* is the distribution symmetric - where does the black line fall in the box

To make comparisons of quantitative variables on a common scale, we can use boxplots


```{r fig.cap="Age broken down by difficulty dressing"}
boxplot(pData$AGEP ~ as.factor(pData$DDRS), col="blue")
```

What the above boxplots don't encode is how many individuals have difficulty dressing vs. ones that don't. We can encode that with `varwidth=TRUE`
```{r cache=TRUE}
boxplot(pData$AGEP ~ as.factor(pData$DDRS), col=c("blue", "orange"), names=c("yes", "no"), varwidth=T)
```
Width is proportional to the number of observations we have in each group.

Barplots
--------
position

```{r}
barplot(table(pData$CIT), col="blue")
```

Histograms
----------
frequency

* params: breaks, freq, col, xlab, ylab, xlim, _ylim, main

```{r cache=TRUE}
hist(pData$AGEP, col="blue")
```

You can see the shape of the distribution as opposed to boxplot.

You can set the number of breaks
```{r cache=T}
hist(pData$AGEP, col="blue", breaks=100, main="Age")
```

Density plots
-------------
Like a histogram, but smoothed out
* params: col, lwd - line thickness, xlab, ylab, xlim, ylim

```{r cache=TRUE}
dens <- density(pData$AGEP)
plot(dens, lwd=3, col="blue")
```

Density is different from frequency in that it shows the percentage of the observations between 0 and 1. Smoothing introduces error

You can overlay distributions for different, say, genders
```{r}
densMales <- density(pData$AGEP[which(pData$SEX == 1)])
plot(dens, lwd=3, col="blue")
lines(densMales, lwd=3, col="orange")
```

Scatterplots
------------
Visualize relationships between variables
* vars: 
 - x, y - points, same length
 - type
 - xlab, ylab - variable labels
 - xlim, ylim - limites
 - cex - size of points, 
 - col - color of point
 - bg - background of points for specific type of points
 - pch - type of point
`?par`

```{r cache=TRUE}
plot(pData$JWMNP, pData$WAGP, pch=19, col="blue")
```

```{r cache=TRUE}
plot(pData$JWMNP, pData$WAGP, pch=19, col="blue", cex=0.5)
```

Encodes color by sex
```{r cache=TRUE}
plot(pData$JWMNP, pData$WAGP, pch=19, col=pData$SEX, cex=0.5)
```

Using size
```{r cache=TRUE}
percentMaxAge <- pData$AGEP/max(pData$AGEP)
plot(pData$JWMNP, pData$WAGP, pch=19, col="blue", cex=percentMaxAge*0.5)
```

overlaying lines/points
```{r cache=TRUE}
plot(pData$JWMNP, pData$WAGP, pch=19, col="blue", cex=0.5)
lines(rep(100, dim(pData)[1]), pData$WAGP, col="grey", lwd=5)
points(seq(0, 200, length=100), seq(0, 20e5, length=100), col="red", pch=19)
```

numeric variables as factors
```{r cache=TRUE}
library(Hmisc)
ageGroups <- cut2(pData$AGEP, g=5)
plot(pData$JWMNP, pData$WAGP, pch=19, col=ageGroups, cex=0.5)
```
`cut2` takes continuous variable and brakes it up into 5 equal intervals and assigns each point as a factor to those intervals.

It doesn't look like there's a difference in distributions across ages for this data.

If there's a lot of points
```{r cache=TRUE}
x <- rnorm(1e5)
y <- rnorm(1e5)
plot(x, y, pch=19)
```
The plot may not be informative. To fix:

sampling
```{r cache=TRUE}
x <- rnorm(1e5)
y <- rnorm(1e5)
sampledValues <- sample(1:1e5, size=1000, replace=FALSE)
plot(x[sampledValues], y[sampledValues], pch=19)
```

smooth scatter - smooth density plot
```{r cache=TRUE}
x <- rnorm(1e5)
y <- rnorm(1e5)
smoothScatter(x, y)
```
outline points are hard dots

hexbin
```{r cache=TRUE}
x <- rnorm(1e5)
y <- rnorm(1e5)
hbo <- hexbin(x, y)
plot(hbo)
```
brakes the set of x and y values into hexagonal bins and then counts the number of variables that are in each hexagonal bin.

QQ-Plots
--------
Takes two variables and plots quantiles of x and y
```{r}
x <- rnorm(20)
y <- rnorm(20)
qqplot(x, y)
# abline takes c(slope, intercept)
abline(c(0, 1))
```
For data that you want to see if it's normally distributed, you can do qq-plot with normal data vs observed data and see if there are large deviation from abline where the data are not normally distributed.

Matplot and spaghetti
---------------------
Observations over time or longitudinal
```{r cache=TRUE}
x <- matrix(rnorm(20*5), nrow=20)
matplot(x, type="b")
```

Heatmaps
--------
Like a 2D histogram
* params: x, y, z, col
```{r cache=TRUE}
image(1:10, 161:236, as.matrix(pData[1:10, 161:236]))
```
White lowest value, red highest.

matching intuition
```{r cache=TRUE}
newMatrix <- as.matrix(pData[1:10, 161:236])
newMatrix <- t(newMatrix)[, nrow(newMatrix):1]
image(161:236, 1:10, newMatrix)
```

Maps
----
```{r cache=TRUE}
library(maps)
map("world")
lat <- runif(40, -180, 180)
lon <- runif(40, -90, 90)
points(lat, lon, col="blue", pch=19)
```

Missing values and plots
------------------------
```{r cache=TRUE}
x <- c(NA, NA, NA, 4, 5, 6, 7, 8, 9, 10)
y <- 1:10
plot(x, y, pch=19, xlim=c(0, 11), ylim=c(0, 11))
```
NA values are skipped, pay attention to make right conclusions.
You can also do a boxplot. Create all values where x < 0 to be NA and see if there's a relationship between them
```{r}
set.seed(8)
x <- rnorm(100)
y <- rnorm(100)
y[x < 0] <- NA
boxplot(x ~ is.na(y))
```
From this we can see that if y == NA, x values are small and when y != NA, x values are big.

Expository Graphs
=================
Those are the graphs included in final analysis.

Axes
----
```{r cache=TRUE}
plot(pData$JWMNP, pData$WAGP, pch=19, col="blue", cex=0.5,
     xlab="Travel time (min)",
     ylab="Last 12 month wages (dollars)")
```

Changing size and orientation
```{r cache=TRUE}
plot(pData$JWMNP, pData$WAGP, pch=19, col="blue", cex=0.5,
     xlab="Travel time (min)",
     ylab="Last 12 month wages (dollars)",
     cex.lab=2,
     cex.axis=1.5)
```

Legends
-------
```{r cache=TRUE}
plot(pData$JWMNP, pData$WAGP, 
     pch=19, col="blue", 
     cex=0.5,
     xlab="Travel time (min)",
     ylab="Last 12 month wages (dollars)")
legend(100, 200000, legend='All surveyed', col="blue", pch=19, cex=0.5)
```

if more than one color
```{r cache=TRUE}
plot(pData$JWMNP, 
     pData$WAGP, 
     pch=19, 
     cex=0.5,
     xlab="Travel time (min)",
     ylab="Wages (dollars)",
     col=pData$SEX)
legend(100, 200000, 
       legend=c("men", "women"), 
       col=c("black", "red"), 
       pch=c(19, 19), cex=c(0.5, 0.5))
```

Titles
------
```{r cache=TRUE}
plot(pData$JWMNP, 
     pData$WAGP, 
     pch=19, 
     cex=0.5,
     xlab="Travel time (min)",
     ylab="Wages (dollars)",
     col=pData$SEX,
     main="Wages earned versus commute time")
legend(100, 200000, 
       legend=c("men", "women"), 
       col=c("black", "red"), 
       pch=c(19, 19), cex=c(0.5, 0.5))
```

Multiple panels
---------------
```{r cache=TRUE}
# orient panels
par(mfrow=c(1, 2))
hist(pData$JWMNP, xlab="CT (min)", col="blue", breaks=100, main="")
plot(pData$JWMNP, 
     pData$WAGP, 
     pch=19, 
     cex=0.5,
     xlab="Travel time (min)",
     ylab="Wages (dollars)",
     col=pData$SEX)
legend(100, 200000, 
       legend=c("men", "women"), 
       col=c("black", "red"), 
       pch=c(19, 19), cex=c(0.5, 0.5))
```

Adding text
-----------
```{r cache=TRUE}
# orient panels
par(mfrow=c(1, 2))
hist(pData$JWMNP, xlab="CT (min)", col="blue", breaks=100, main="")
mtext(text="(a)", side=3, line=1)
plot(pData$JWMNP, 
     pData$WAGP, 
     pch=19, 
     cex=0.5,
     xlab="Travel time (min)",
     ylab="Wages (dollars)",
     col=pData$SEX)
legend(100, 200000, 
       legend=c("men", "women"), 
       col=c("black", "red"), 
       pch=c(19, 19), cex=c(0.5, 0.5))
mtext(text="(b)", side=3, line=1)
```

Figure captions
---------------
slide 13/21

Graphical Workflow
------------------
**PDF**
* params: file, height, width

```
pdf(file="twoPanel.pdf", height=480, width=(2 * 480))
par(mfrow=c(1, 2))
hist(pData$JWMNP, xlab="CT (min)", col="blue", breaks=100, main="")
mtext(text="(a)", side=3, line=1)
plot(pData$JWMNP, 
     pData$WAGP, 
     pch=19, 
     cex=0.5,
     xlab="Travel time (min)",
     ylab="Wages (dollars)",
     col=pData$SEX)
legend(100, 200000, 
       legend=c("men", "women"), 
       col=c("black", "red"), 
       pch=c(19, 19), cex=c(0.5, 0.5))
mtext(text="(b)", side=3, line=1)
dev.off()
```

Same for **PNG**

You can tweak graph and copy it directly to pdf 

```
par(mfrow=c(1, 2))
hist(pData$JWMNP, xlab="CT (min)", col="blue", breaks=100, main="")
plot(pData$JWMNP, 
     pData$WAGP, 
     pch=19, 
     cex=0.5,
     xlab="Travel time (min)",
     ylab="Wages (dollars)",
     col=pData$SEX)
dev.copy2pdf(file="twoPanelv2.pdf")
```

Hierarchical clustering
-----------------------
Organizes things that are **close** into groups
produces dendrogram

How do we define close?
* most important step
 - Garbage in  -> garbage out: if we use a garbage variable, we'll produce garbage clustering
 
Euclidean distance
* Used for quantitative - continuous vairables

Manhattan distance
* distance on a grid (?)

example
```{r cache=TRUE}
set.seed(1234)
par(mar=c(0, 0, 0, 0))
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean=rep(1, 2, 1, each=4), sd=0.2)
plot(x, y, col="blue", pch=19, cex=2)
text(x + 0.05, y + 0.05, labels=as.character(1:12))

```

dist
```{r cache=TRUE}
dataFrame <- data.frame(x=x, y=y)
dist(dataFrame)
```

#1
closest 2 points get picked, and merged, and so forth until we have 1 point left
```{r cache=TRUE}
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
plot(hClustering)
```

To decide clusters things get broken into, we have to cut the dendrogram at some point.

#### Prettier dendrograms
```{r cache=TRUE}
myplclust <- function(hclust,
                      lab=hclust$labels,
                      lab.col=rep(1, length(hclust$labels)),
                      hang=0.1,
                      ...){
  y <- rep(hclust$height, 2)
  x <- as.numeric(hclust$merge)
  y <- y[which(x < 0)]
  x <- x[which(x < 0)]
  x <- abs(x)
  y <- y[order(x)]
  x <- x[order(x)]
  plot(hclust, labels=F, hang=hang, ...)
  text(x=x, y=y[hclust$order] - (max(hclust$height) * hang),
       labels=lab[hclust$order], col=lab.col[hclust$order],
       srt=90, adj=c(1, 0.5), xpd=NA, ...)
}
dataFrame <- data.frame(x=x, y=y)
distxy <- dist(dataFrame)
hClustering <- hclust(distxy)
myplclust(hClustering, lab=rep(1:3, each=4), lab.col=rep(1:3, each=4))
```

graph 79 on R Graph Gallery

Merging points:
* complete
 - Complete linkage - comparing points farthest apart
* average
 - uses average distance
 
#### heatmap()
visualizing quantitative data after they were clustered

```{r cache=TRUE}
df <- data.frame(x=x, y=y)
set.seed(143)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
heatmap(dataMatrix)
```

K-Means Clustering
==================
Continuous - Euclidian distance is usually used in k-means clustering.

When determining a reasonable number of clusters, pick a large one, then go down.

```{r cache=TRUE}
set.seed(1234)
par(mar=c(0,0,0,0))
x <- rnorm(12, mean=rep(1:3, each=4), sd=0.2)
y <- rnorm(12, mean=rep(c(1, 2, 1), each=4), sd=0.2)
plot(x, y, col="blue", pch=19, cex=2)
text(x + 0.05, y + 0.05, labels=as.character(1:12))
```
#### Starting centroids
1. Assign all the points to the closer centroid
2. Recalculate centroids by taking the euclidian distance between each of the points
3. Reassign points to centroids
4. Iterate

`kmeans()`
* params: 
 - x - data frame or matrix
 - centers - number of centers
 - iter.max - for large datasets not to run forever
 - nstart - number of starting points - kmeans is non-deterministic, you can restart multiple times and get an average cluster over those starts
```{r cache=TRUE}
dataFrame <- data.frame(x, y)
kmeansObj <- kmeans(dataFrame, centers=3)
names(kmeansObj)
```

```{r cache=TRUE}
kmeansObj$cluster
```

#### Plot clusters
```{r cache=TRUE}
par(mar=rep(0.2, 4))
plot(x, y, col=kmeansObj$cluster, pch=19, cex=2)
points(kmeansObj$centers, col=1:3, pch=3, cex=3, lwd=3)
```

#### With heatmaps
```{r cache=TRUE}
set.seed(1234)
dataMatrix <- as.matrix(dataFrame)[sample(1:12),]
kmeansObj2 <- kmeans(dataMatrix, centers=3)
par(mfrow=c(1, 2), mar=rep(0.2, 4))
image(t(dataMatrix)[,nrow(dataMatrix):1], yaxt="n")
image(t(dataMatrix)[,order(kmeansObj$cluster)], yaxt="n")
```

Scrambled and unscrambled heatmaps - col1: x, col2: y

Principal components analysis and singular value decomposition
==============================================================
Compress large set of variables into a smaller one

Matrix data
-----------
No relation between variables and observation
column: variable
row: observation
```{r cache=TRUE}
set.seed(12345)
par(mar=rep(0.2, 4))
dataMatrix <- matrix(rnorm(400), nrow=40)
image(1:10, 1:40, t(dataMatrix)[,nrow(dataMatrix):1])
```

You can confirm there are no patterns with dendrogram
```{r cache=TRUE}
par(mar=rep(0.2, 4))
heatmap(dataMatrix)
```

We can add a pattern that affects some rows
```{r cache=TRUE}
# for each row
for(i in 1:40){
  # flip a coin
  coinFlip <- rbinom(1, size=1, prob=0.5)
  # if coin is heads add a common pattern to that row
  if(coinFlip){
    # pattern: first 5 variables are 0, last 5 are 3
    dataMatrix[i,] <- dataMatrix[i,] + rep(c(0, 3), each=5)
  }
}
```

```{r cache=TRUE}
par(mar=rep(0.2, 4))
heatmap(dataMatrix)
```

To find the patterns without looking at the entire data set we can:

1. take the means of rows and columns

```{r cache=TRUE}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
par(mfrow=c(1, 3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rowMeans(dataMatrixOrdered), 40:1,, 
     xlab="Row",
     ylab="Row Mean",
     pch=19)
plot(colMeans(dataMatrixOrdered),
     xlab="Column",
     ylab="Column Mean",
     pch=19)
```
In figure 2 mean of each row is plotted, we can see hihger values cluster on the top right.

What happens if there's more than one pattern?

SVD
---
#### Components: v
```{r cache=TRUE}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1, 3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd1$u[,1], 40:1,, 
     xlab="Row",
     ylab="First left singular vector - u",
     pch=19)
plot(svd1$v[,1],
     xlab="Column",
     ylab="First right singular vector - v",
     pch=19)
```

Looking at v - the 5 values at the bottom right make sense because last 5 columns all have a particular value, unlike first 5.

#### Components: d and variance
```{r cache=TRUE}
svd1 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1, 2))
plot(svd1$d, 
     xlab="Column",
     ylab="Singular value",
     pch=19)
plot(svd1$d^2/sum(svd1$d^2),
     xlab="Column",
     ylab="Percent of variance explained",
     pch=19)
```
The decreasing set of values in the first figure start with first left singular vector, then second, etc.

Figure 2 - first point explains 40 percent of the variance, etc. What people do is to add up the cumulative amounts of variance explained moving from left to right and identify patterns that explain some large percentage of the variance, 80, 90%. They can then compress the data by only considering components of SVD that represent those patterns.

Relationship to PCA
-------------------
Taking first PC vs first right singular vector, they're the same
```{r cache=TRUE}
svd1 <- svd(scale(dataMatrixOrdered))
pca1 <- prcomp(dataMatrixOrdered, scale=T)
plot(pca1$rotation[,1], svd1$v[,1], pch=19,
     xlab="Principal Component 1",
     ylab="Right Singular Vector 1")
abline(c(0, 1))
```

Components of the SVD - variance explained
------------------------------------------
```{r cache=TRUE}
constantMatrix <- dataMatrixOrdered * 0
for(i in 1:dim(dataMatrixOrdered)[1]) {
  # matrix where every row is the same
  constantMatrix[i,] <- rep(c(0, 1), each=5)
}
svd1 <- svd(constantMatrix)
par(mfrow=c(1, 3))
image(t(constantMatrix)[,nrow(constantMatrix):1])
plot(svd1$d, xlab="Column", ylab="Singular value", pch=19)
plot(svd1$d^2/sum(svd1$d^2),
     xlab="Column",
     ylab="Percent of variance explained",
     pch=19)
```

100% of the variance is explained by the first d value. Reason is that if we take u and v and multiply them together, we can reproduce the matrix exactly, so all the variation in the matrix is explained by one pattern.

If the data was random, each singular vector would explain approximately 1/the number of columns variance in the data set.

What if we add a second pattern?
--------------------------------
```{r cache=TRUE}
set.seed(678910)
for(i in 1:40){
  # flip a coin
  coinFlip1 <- rbinom(1, size=1, prob=0.5)
  coinFlip2 <- rbinom(1, size=1, prob=0.5)
  
  # if coin is heads, add a common pattern to that row
  if(coinFlip1){
    dataMatrix[i,] <- dataMatrix[i, ] + rep(c(0, 5), each=5)
  }
  if(coinFlip2){
    dataMatrix[i,] <- dataMatrix[i, ] + rep(c(0, 5), 5)
  }
}
hh <- hclust(dist(dataMatrix))
dataMatrixOrdered <- dataMatrix[hh$order,]
```

Now cluster the data set and plot
```{r cache=TRUE}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1, 3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(rep(c(0, 1), each=5), 
     xlab="Column",
     ylab="Pattern 1",
     pch=19)
plot(rep(c(0, 1), 5),
     xlab="Column",
     ylab="Pattern 2",
     pch=19)
```
We can see that there are 2 patterns in the data set:
* values high on right hand side and low on left hand side
* stripes - low - high - low - high

The two patterns overlap each other

In the second plot, we can see Pattern 1 is low for first 5 columns then high for next 5. Pattern 2 is flip/flopping.

v and patterns of variance in rows
----------------------------------
svd2 <- svd(scale(dataMatrixOrdered))
```{r cache=TRUE}
par(mfrow=c(1, 3))
image(t(dataMatrixOrdered)[,nrow(dataMatrixOrdered):1])
plot(svd2$v[,1], 
     xlab="Column",
     ylab="First right singular vector",
     pch=19)
plot(svd2$v[,2],
     xlab="Column",
     ylab="Second right singular vector",
     pch=19)
```
The first figure flip flops low/high, but also first 5 values are lower than second 5. In second figure, the patterns are flipped. Together they explain most of the variation in the matrix.

d and variance explained
------------------------
```{r cache=TRUE}
svd2 <- svd(scale(dataMatrixOrdered))
par(mfrow=c(1, 2))
plot(svd2$d,
     xlab="Column",
     ylab="Singular value",
     pch=19)
plot(svd2$d^2/sum(svd1$d^2),
     xlab="Column",
     ylab="Percent of variance explained",
     pch=19)
```

We can see that first and second values explain most of the variance. This would suggest that there are two patterns in the data set but they aren't necessarily the two patterns we observed in singular vectors. Each of the singular vectors may represent multiple patterns mashed together.

fast.svd function {corpcor}
---------------------------
* params: m, tol
```{r cache=TRUE}
bigMatrix <- matrix(rnorm(1e4 * 40), nrow=1e4)
system.time(svd(scale(bigMatrix)))
```
```{r cache=TRUE}
library(corpcor)
# fast.svd only calculates singular values/vectors for values above the tolerance, this way it calculates all
system.time(fast.svd(scale(bigMatrix), tol=0))
```

Missing values
--------------
SVD cannot be performed on missing values, 19/25.
One option is to impute those values

Imputing {impute}
-----------------
```
source("http://bioconductor.org/biocLite.R")
biocLite("impute")
then install from directory
to cite: citation("impute")
```
There's a bunch of ways but for exploratory analysis use impute package. knn approach
```{r cache=TRUE}
library(impute)
dataMatrix2 <- dataMatrixOrdered
dataMatrix2[sample(1:100, size=40, replace=F)] <- NA
dataMatrix2 <- impute.knn(dataMatrix2)$data
svd1 <- svd(scale(dataMatrixOrdered))
svd2 <- svd(scale(dataMatrix2))
par(mfrow=c(1, 2))
plot(svd1$v[,1], pch=19)
plot(svd2$v[,1], pch=19)
```
Compared to the complete data set, they're very similar, however if the dataset is missing a large number of data, it may not always work.

Face example
-------------
21/25
Advanced data analysis from an elementary point of view
Elements of statistical learning